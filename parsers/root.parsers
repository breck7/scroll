abstractScrollParser
 atoms cueAtom
 javascript
  buildHtmlSnippet(buildSettings) {
   return this.buildHtml(buildSettings)
  }
  buildTxt() {
    return ""
  }
  getHtmlRequirements(buildSettings) {
    const {requireOnce} = this
    if (!requireOnce)
      return ""
    const set = buildSettings?.alreadyRequired || this.root.alreadyRequired
    if (set.has(requireOnce))
      return ""
    set.add(requireOnce)
    return requireOnce + "\n\n"
  }

abstractScrollWithRequirementsParser
 extends abstractScrollParser
 cueFromId
 javascript
  buildHtml(buildSettings) {
    return this.getHtmlRequirements(buildSettings) + this.buildInstance()
  }

metaCommandAtom
 extends cueAtom
 description Give meta command atoms their own color.
 paint constant.numeric
 // Obviously this is not numeric. But I like the green color for now.
   We need a better design to replace this "paint" concept
   https://github.com/breck7/scrollsdk/issues/186

abstractTopLevelSingleMetaParser
 description Use these parsers once per file.
 extends abstractScrollParser
 inScope slashCommentParser
 cueFromId
 atoms metaCommandAtom
 javascript
  isTopMatter = true
  isSetterParser = true
  buildHtml() {
   return ""
  }

scrollParser
 description Scroll is a language for scientists of all ages. Refine, share and collaborate on ideas.
 root
 inScope abstractScrollParser blankLineParser atomTypeDefinitionParser parserDefinitionParser
 catchAllParser catchAllParagraphParser
 javascript
  setFile(file) {
   this.file = file
   const date = this.get("date")
   if (date) this.file.timestamp = this.dayjs(this.get("date")).unix()
   return this
  }
  buildHtml(buildSettings) {
    this.sectionStack = []
    return this.filter(subparticle => subparticle.buildHtml).map(subparticle => { try {return subparticle.buildHtml(buildSettings)} catch (err) {console.error(err); return ""} }).filter(i => i).join("\n") + this.clearSectionStack()
  }
  sectionStack = []
  clearSectionStack() {
   const result = this.sectionStack.join("\n")
   this.sectionStack = []
   return result
  }
  bodyStack = []
  clearBodyStack() {
   const result = this.bodyStack.join("")
   this.bodyStack = []
   return result
  }
  get hakonParser() {
    if (this.isNodeJs())
      return require("scrollsdk/products/hakon.nodejs.js")
    return hakonParser
  }
  readSyncFromFileOrUrl(fileOrUrl) {
    if (!this.isNodeJs()) return this.getInBrowser(fileOrUrl)
    const isUrl = fileOrUrl.match(/^https?\:[^ ]+$/)
    if (!isUrl) return this.root.readFile(fileOrUrl)
    return this.readFile(this.makeFullPath(new URL(fileOrUrl).pathname.split('/').pop()))
  }
  getInBrowser(key) {
    return localStorage.getItem(key) || (window.inMemStorage ? window.inMemStorage[key] : "") || ""
  }
  async fetch(url, filename) {
    const isUrl = url.match(/^https?\:[^ ]+$/)
    if (!isUrl) return
    return this.isNodeJs() ? this.fetchNode(url, filename) : this.fetchBrowser(url)
  }
  get path() {
    return require("path")
  }
  makeFullPath(filename) {
    return this.path.join(this.folderPath, filename)
  }
  _nextAndPrevious(arr, index) {
    const nextIndex = index + 1
    const previousIndex = index - 1
    return {
      previous: arr[previousIndex] ?? arr[arr.length - 1],
      next: arr[nextIndex] ?? arr[0]
    }
  }
  // keyboard nav is always in the same folder. does not currently support cross folder
  includeFileInKeyboardNav(file) {
    const { scrollProgram } = file
    return scrollProgram.buildsHtml && scrollProgram.hasKeyboardNav && scrollProgram.tags.includes(this.primaryTag)
  }
  get timeIndex() {
    return this.file.timeIndex || 0
  }
  get linkToPrevious() {
    if (!this.hasKeyboardNav)
      // Dont provide link to next unless keyboard nav is on
      return undefined
    const {allScrollFiles} = this
    let file = this._nextAndPrevious(allScrollFiles, this.timeIndex).previous
    while (!this.includeFileInKeyboardNav(file)) {
      file = this._nextAndPrevious(allScrollFiles, file.timeIndex).previous
    }
    return file.scrollProgram.permalink
  }
  importRegex = /^(import |[a-zA-Z\_\-\.0-9\/]+\.(scroll|parsers)$|https?:\/\/.+\.(scroll|parsers)$)/gm
  get linkToNext() {
    if (!this.hasKeyboardNav)
      // Dont provide link to next unless keyboard nav is on
      return undefined
    const {allScrollFiles} = this
    let file = this._nextAndPrevious(allScrollFiles, this.timeIndex).next
    while (!this.includeFileInKeyboardNav(file)) {
      file = this._nextAndPrevious(allScrollFiles, file.timeIndex).next
    }
    return file.scrollProgram.permalink
  }
  // todo: clean up this naming pattern and add a parser instead of special casing 404.html
  get allHtmlFiles() {
    return this.allScrollFiles.filter(file => file.scrollProgram.buildsHtml && file.scrollProgram.permalink !== "404.html")
  }
  parseNestedTag(tag) {
    if (!tag.includes("/")) return;
    const {path} = this
    const parts = tag.split("/")
    const group = parts.pop()
    const relativePath = parts.join("/")
    return {
      group,
      relativePath,
      folderPath: path.join(this.folderPath, path.normalize(relativePath))
      }
  }
  getFilesByTags(tags, limit) {
    // todo: tags is currently matching partial substrings
    const getFilesWithTag = (tag, files) => files.filter(file => file.scrollProgram.buildsHtml && file.scrollProgram.tags.includes(tag))
    if (typeof tags === "string") tags = tags.split(" ")
    if (!tags || !tags.length)
      return this.allHtmlFiles
        .filter(file => file !== this) // avoid infinite loops. todo: think this through better.
        .map(file => {
          return { file, relativePath: "" }
        })
        .slice(0, limit)
    let arr = []
    tags.forEach(tag => {
      if (!tag.includes("/"))
        return (arr = arr.concat(
          getFilesWithTag(tag, this.allScrollFiles)
            .map(file => {
              return { file, relativePath: "" }
            })
            .slice(0, limit)
        ))
      const {folderPath, group, relativePath} = this.parseNestedTag(tag)
      let files = []
      try {
      files = this.fileSystem.getCachedLoadedFilesInFolder(folderPath, this)
      } catch (err) {
        console.error(err)
      }
      const filtered = getFilesWithTag(group, files).map(file => {
        return { file, relativePath: relativePath + "/" }
      })
      arr = arr.concat(filtered.slice(0, limit))
    })
    return this.lodash.sortBy(arr, file => file.file.timestamp).reverse()
  }
  async fetchNode(url, filename) {
    filename = filename || new URL(url).pathname.split('/').pop()
    const fullpath = this.makeFullPath(filename)
    if (require("fs").existsSync(fullpath)) return this.readFile(fullpath)
    this.log(`ðŸ›œ fetching ${url} to ${fullpath} `)
    await this.downloadToDisk(url, fullpath)
    return this.readFile(fullpath)
  }
  log(message) {
    if (this.logger) this.logger.log(message)
  }
  async fetchBrowser(url) {
    const content = this.getInBrowser(url)
    if (content) return content
    return this.downloadToLocalStorage(url)
  }
  async downloadToDisk(url, destination) {
    const { writeFile } = require('fs').promises
    const response = await fetch(url)
    const fileBuffer = await response.arrayBuffer()
    await writeFile(destination, Buffer.from(fileBuffer))
    return this.readFile(destination)
  }
  async downloadToLocalStorage(url) {
    const response = await fetch(url)
    const blob = await response.blob()
    const text = await blob.text()
    try {
      localStorage.setItem(url, text)
      return localStorage.getItem(url)
    } catch (err) {
      if (!window.inMemStorage) window.inMemStorage = {}
      window.inMemStorage[url] = text
      console.error(err)
      return text
    }
  }
  readFile(filename) {
    const {path} = this
    const fs = require("fs")
    const fullPath = path.join(this.folderPath, filename.replace(this.folderPath, ""))
    try {
    if (fs.existsSync(fullPath))
      return fs.readFileSync(fullPath, "utf8")
    console.error(`File '${filename}' not found`)
    return ""
    } catch (err) {
      console.error(`Error in '${this.filePath}' reading file: '${fullPath}'`)
      console.error(err)
      return ""
    }
  }
  alreadyRequired = new Set()
  buildHtmlSnippet(buildSettings) {
   this.sectionStack = []
   return this.map(subparticle => (subparticle.buildHtmlSnippet ? subparticle.buildHtmlSnippet(buildSettings) : subparticle.buildHtml(buildSettings)))
     .filter(i => i)
     .join("\n")
     .trim() + this.clearSectionStack()
  }
  get footnotes() {
   if (this._footnotes === undefined) this._footnotes = this.filter(particle => particle.isFootnote)
   return this._footnotes
  }
  get authors() {
    return this.get("authors")
  }
  get allScrollFiles() {
    try {
    return this.fileSystem.getCachedLoadedFilesInFolder(this.folderPath, this)
    } catch (err) {
      console.error(err)
      return []
    }
  }
  async doThing(thing) {
    await Promise.all(this.filter(particle => particle[thing]).map(async particle => particle[thing]()))
  }
  async load() {
    await this.doThing("load")
  }
  async execute() {
    await this.doThing("execute")
  }
  file = {}
  getFromParserId(parserId) {
    return this.parserIdIndex[parserId]?.[0].content
  }
  get fileSystem() {
    return this.file.fileSystem
  }
  get filePath() {
    return this.getLine()
  }
  get folderPath() {
    return Utils.posix.dirname(this.filePath) + "/"
  }
  get filename() {
    return Utils.posix.basename(this.filePath)
  }
  get hasKeyboardNav() {
    return this.has("keyboardNav")
  }
  get editHtml() {
    return `<a href="${this.editUrl}" class="abstractTextLinkParser">Edit</a>`
  }
  get externalsPath() {
    return this.path.join(this.fileSystem.standardParserDirectory, "..", "external")
  }
  get endSnippetIndex() {
    // Get the line number that the snippet should stop at.
    // First if its hard coded, use that
    if (this.has("endSnippet")) return this.getParticle("endSnippet").index
    // Next look for a dinkus
    const snippetBreak = this.find(particle => particle.isDinkus)
    if (snippetBreak) return snippetBreak.index
    return -1
  }
  get parserIds() {
    return this.topDownArray.map(particle => particle.definition.id)
  }
  get tags() {
    return this.get("tags") || ""
  }
  get primaryTag() {
    return this.tags.split(" ")[0]
  }
  get filenameNoExtension() {
    return this.filename.replace(".scroll", "")
  }
  // todo: rename publishedUrl? Or something to indicate that this is only for stuff on the web (not localhost)
  // BaseUrl must be provided for RSS Feeds and OpenGraph tags to work
  get baseUrl() {
    const baseUrl = (this.get("baseUrl") || "").replace(/\/$/, "")
    return baseUrl + "/"
  }
  get canonicalUrl() {
    return this.get("canonicalUrl") || this.baseUrl + this.permalink
  }
  get openGraphImage() {
    const openGraphImage = this.get("openGraphImage")
    if (openGraphImage !== undefined) return this.ensureAbsoluteLink(openGraphImage)
    const images = this.filter(particle => particle.doesExtend("scrollImageParser"))
    const hit = images.find(particle => particle.has("openGraph")) || images[0]
    if (!hit) return ""
    return this.ensureAbsoluteLink(hit.filename)
  }
  get absoluteLink() {
    return this.ensureAbsoluteLink(this.permalink)
  }
  ensureAbsoluteLink(link) {
    if (link.includes("://")) return link
    return this.baseUrl + link.replace(/^\//, "")
  }
  get editUrl() {
    const editUrl = this.get("editUrl")
    if (editUrl) return editUrl
    const editBaseUrl = this.get("editBaseUrl")
    return (editBaseUrl ? editBaseUrl.replace(/\/$/, "") + "/" : "") + this.filename
  }
  get gitRepo() {
    // given https://github.com/breck7/breckyunits.com/blob/main/four-tips-to-improve-communication.scroll
    // return https://github.com/breck7/breckyunits.com
    return this.editUrl.split("/").slice(0, 5).join("/")
  }
  get scrollVersion() {
    // currently manually updated
    return "170.4.0"
  }
  // Use the first paragraph for the description
  // todo: add a particle method version of get that gets you the first particle. (actulaly make get return array?)
  // would speed up a lot.
  get description() {
    const description = this.getFromParserId("openGraphDescriptionParser")
    if (description) return description
    return this.generatedDescription
  }
  get keywords() {
    if (this.has("keywords"))
    return this.get("keywords")
    const tags = this.get("tags")
    if (tags)
      return tags.split(" ").join(", ")
    return ""
  }
  get generatedDescription() {
    const firstParagraph = this.find(particle => particle.isArticleContent)
    return firstParagraph ? firstParagraph.originalText.substr(0, 100).replace(/[&"<>']/g, "") : ""
  }
  get titleFromFilename() {
    const unCamelCase = str => str.replace(/([a-z])([A-Z])/g, "$1 $2").replace(/^./, match => match.toUpperCase())
    return unCamelCase(this.filenameNoExtension)
  }
  get title() {
    return this.getFromParserId("scrollTitleParser") || this.titleFromFilename
  }
  get linkTitle() {
    return this.getFromParserId("scrollLinkTitleParser") || this.title
  }
  get permalink() {
   return this.get("permalink") || (this.filename ? this.filenameNoExtension + ".html" : "")
  }
  compileTo(extensionCapitalized) {
    if (extensionCapitalized === "Txt")
      return this.asTxt
    if (extensionCapitalized === "Html")
      return this.asHtml
    const methodName = "build" + extensionCapitalized
    return this.topDownArray
      .filter(particle => particle[methodName])
      .map((particle, index) => particle[methodName](index))
      .join("\n")
      .trim()
  }
  get asTxt() {
    return (
      this.map(particle => {
          const text = particle.buildTxt ? particle.buildTxt() : ""
          if (text) return text + "\n"
          if (!particle.getLine().length) return "\n"
          return ""
        })
        .join("")
        .replace(/\n\n\n+/g, "\n\n") // Maximum 2 newlines in a row
        .trim() + "\n" // Always end in a newline, Posix style
    )
  }
  get dependencies() {
      const dependencies = this.file.dependencies?.slice() || []
      const files = this.topDownArray.filter(particle => particle.dependencies).map(particle => particle.dependencies).flat()
      return dependencies.concat(files)
  }
  get buildsHtml() {
    const { permalink } = this
    return !this.file.importOnly && (permalink.endsWith(".html") || permalink.endsWith(".htm"))
  }
  // Without specifying the language hyphenation will not work.
  get lang() {
    return this.get("htmlLang") || "en"
  }
  _compiledHtml = ""
  get asHtml() {
    if (!this._compiledHtml) {
      const { permalink, buildsHtml } = this
      const content = (this.buildHtml() + this.clearBodyStack()).trim()
      // Don't add html tags to CSV feeds. A little hacky as calling a getter named _html_ to get _xml_ is not ideal. But
      // <1% of use case so might be good enough.
      const wrapWithHtmlTags = buildsHtml
      const bodyTag = this.has("metaTags") ? "" : "<body>\n"
      this._compiledHtml = wrapWithHtmlTags ? `<!DOCTYPE html>\n<html lang="${this.lang}">\n${bodyTag}${content}\n</body>\n</html>` : content
    }
    return this._compiledHtml
  }
  get wordCount() {
    return this.asTxt.match(/\b\w+\b/g)?.length || 0
  }
  get minutes() {
    return parseFloat((this.wordCount / 200).toFixed(1))
  }
  get date() {
    const date = this.get("date") || (this.file.timestamp ? this.file.timestamp : 0)
    return this.dayjs(date).format(`MM/DD/YYYY`)
  }
  get year() {
    return parseInt(this.dayjs(this.date).format(`YYYY`))
  }
  get dayjs() {
    if (!this.isNodeJs()) return dayjs
    const lib = require("dayjs")
    const relativeTime = require("dayjs/plugin/relativeTime")
    lib.extend(relativeTime)
    return lib
  }
  get lodash() {
    return this.isNodeJs() ? require("lodash") : lodash
  }
  get d3() {
    return this.isNodeJs() ? require('d3') : d3
  }
  getConcepts(parsed) {
    const concepts = []
    let currentConcept
    parsed.forEach(particle => {
      if (particle.isConceptDelimiter) {
        if (currentConcept) concepts.push(currentConcept)
        currentConcept = []
      }
      if (currentConcept && particle.isMeasure) currentConcept.push(particle)
    })
    if (currentConcept) concepts.push(currentConcept)
    return concepts
  }
  _formatConcepts(parsed) {
    const concepts = this.getConcepts(parsed)
    if (!concepts.length) return false
    const {lodash} = this
    // does a destructive sort in place on the parsed program
    concepts.forEach(concept => {
      let currentSection
      const newCode = lodash
        .sortBy(concept, ["sortIndex"])
        .map(particle => {
          let newLines = ""
          const section = particle.sortIndex.toString().split(".")[0]
          if (section !== currentSection) {
            currentSection = section
            newLines = "\n"
          }
          return newLines + particle.toString()
        })
        .join("\n")
      concept.forEach((particle, index) => (index ? particle.destroy() : ""))
      concept[0].replaceParticle(() => newCode)
    })
  }
  get formatted() {
    return this.getFormatted(this.file.codeAtStart)
  }
  get lastCommitTime() {
    // todo: speed this up and do a proper release. also could add more metrics like this.
    if (this._lastCommitTime === undefined) {
      try {
        this._lastCommitTime = require("child_process").execSync(`git log -1 --format="%at" -- "${this.filePath}"`).toString().trim()
      } catch (err) {
        this._lastCommitTime = 0
      }
    }
    return this._lastCommitTime
  }
  getFormatted(codeAtStart = this.toString()) {
    let formatted = codeAtStart.replace(/\r/g, "") // remove all carriage returns if there are any
    const parsed = new this.constructor(formatted)
    parsed.topDownArray.forEach(subparticle => {
      subparticle.format()
      const original = subparticle.getLine()
      const trimmed = original.replace(/(\S.*?)[  \t]*$/gm, "$1")
      // Trim trailing whitespace unless parser allows it
      if (original !== trimmed && !subparticle.allowTrailingWhitespace) subparticle.setLine(trimmed)
    })
    this._formatConcepts(parsed)
    let importOnlys = []
    let topMatter = []
    let allElse = []
    // Create any bindings
    parsed.forEach(particle => {
      if (particle.bindTo === "next") particle.binding = particle.next
      if (particle.bindTo === "previous") particle.binding = particle.previous
    })
    parsed.forEach(particle => {
      if (particle.getLine() === "importOnly") importOnlys.push(particle)
      else if (particle.isTopMatter) topMatter.push(particle)
      else allElse.push(particle)
    })
    const combined = importOnlys.concat(topMatter, allElse)
    // Move any bound particles
    combined
      .filter(particle => particle.bindTo)
      .forEach(particle => {
        // First remove the particle from its current position
        const originalIndex = combined.indexOf(particle)
        combined.splice(originalIndex, 1)
        // Then insert it at the new position
        // We need to find the binding index again after removal
        const bindingIndex = combined.indexOf(particle.binding)
        if (particle.bindTo === "next") combined.splice(bindingIndex, 0, particle)
        else combined.splice(bindingIndex + 1, 0, particle)
      })
    const trimmed = combined
      .map(particle => particle.toString())
      .join("\n")
      .replace(/^\n*/, "") // Remove leading newlines
      .replace(/\n\n\n+/g, "\n\n") // Maximum 2 newlines in a row
      .replace(/\n+$/, "")
    return trimmed === "" ? trimmed : trimmed + "\n" // End non blank Scroll files in a newline character POSIX style for better working with tools like git
  }
  get parser() {
    return this.constructor
  }
  get parsersRequiringExternals() {
    const { parser } = this
    // todo: could be cleaned up a bit
    if (!parser.parsersRequiringExternals) parser.parsersRequiringExternals = parser.cachedHandParsersProgramRoot.filter(particle => particle.copyFromExternal).map(particle => particle.atoms[0])
    return parser.parsersRequiringExternals
  }
  get Disk() { return this.isNodeJs() ? require("scrollsdk/products/Disk.node.js").Disk : {}}
  async buildAll(options = {}) {
    await this.load()
    await this.buildOne(options)
    await this.buildTwo(options)
  }
  async buildOne(options) {
    await this.execute()
    const toBuild = this.filter(particle => particle.buildOne)
    for (let particle of toBuild) {
      await particle.buildOne(options)
    }
  }
  async buildTwo(options) {
    const toBuild = this.filter(particle => particle.buildTwo)
    for (let particle of toBuild) {
      await particle.buildTwo(options)
    }
  }
  get outputFileNames() {
    return this.filter(p => p.outputFileNames).map(p => p.outputFileNames).flat()
  }
  _compileArray(filename, arr) {
    const removeBlanks = data => data.map(obj => Object.fromEntries(Object.entries(obj).filter(([_, value]) => value !== "")))
    const parts = filename.split(".")
    const format = parts.pop()
    if (format === "json") return JSON.stringify(removeBlanks(arr), null, 2)
    if (format === "js") return `const ${parts[0]} = ` + JSON.stringify(removeBlanks(arr), null, 2)
    if (format === "csv") return this.arrayToCSV(arr)
    if (format === "tsv") return this.arrayToCSV(arr, "\t")
    if (format === "particles") return particles.toString()
    return particles.toString()
  }
  levenshteinDistance(a, b) {
    const m = a.length
    const n = b.length
    const dp = Array.from({ length: m + 1 }, () => Array(n + 1).fill(0))
    for (let i = 0; i <= m; i++) {
      dp[i][0] = i
    }
    for (let j = 0; j <= n; j++) {
      dp[0][j] = j
    }
    for (let i = 1; i <= m; i++) {
      for (let j = 1; j <= n; j++) {
        const cost = a[i - 1] === b[j - 1] ? 0 : 1
        dp[i][j] = Math.min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)
      }
    }
    return dp[m][n]
  }
  makeLodashOrderByParams(str) {
  const part1 = str.split(" ")
  const part2 = part1.map(col => (col.startsWith("-") ? "desc" : "asc"))
  return [part1.map(col => col.replace(/^\-/, "")), part2]
  }
  arrayToCSV(data, delimiter = ",") {
   if (!data.length) return ""
   // Extract headers
   const headers = Object.keys(data[0])
   const csv = data.map(row =>
     headers
       .map(fieldName => {
         const fieldValue = row[fieldName]
         // Escape commas if the value is a string
         if (typeof fieldValue === "string" && fieldValue.includes(delimiter)) {
           return `"${fieldValue.replace(/"/g, '""')}"` // Escape double quotes and wrap in double quotes
         }
         return fieldValue
       })
       .join(delimiter)
   )
   csv.unshift(headers.join(delimiter)) // Add header row at the top
   return csv.join("\n")
   }
  compileConcepts(filename = "csv", sortBy = "") {
    const {lodash} = this
    if (!sortBy) return this._compileArray(filename, this.concepts)
    const orderBy = this.makeLodashOrderByParams(sortBy)
    return this._compileArray(filename, lodash.orderBy(this.concepts, orderBy[0], orderBy[1]))
  }
  _withStats
  get measuresWithStats() {
    if (!this._withStats) this._withStats = this.addMeasureStats(this.concepts, this.measures)
    return this._withStats
  }
  addMeasureStats(concepts, measures){
   return measures.map(measure => {
    let Type = false
    concepts.forEach(concept => {
      const value = concept[measure.Name]
      if (value === undefined || value === "") return
      measure.Values++
      if (!Type) {
        measure.Example = value.toString().replace(/\n/g, " ")
        measure.Type = typeof value
        Type = true
      }
    })
    measure.Coverage = Math.floor((100 * measure.Values) / concepts.length) + "%"
    return measure
  })
  }
  parseMeasures(parser) {
  if (!Particle.measureCache)
    Particle.measureCache = new Map()
  const measureCache = Particle.measureCache
  if (measureCache.get(parser)) return measureCache.get(parser)
  const {lodash} = this
  // todo: clean this up
  const getCueAtoms = rootParserProgram =>
    rootParserProgram
      .filter(particle => particle.getLine().endsWith("Parser") && !particle.getLine().startsWith("abstract"))
      .map(particle => particle.get("cue") || particle.getLine())
      .map(line => line.replace(/Parser$/, ""))
  // Generate a fake program with one of every of the available parsers. Then parse it. Then we can easily access the meta data on the parsers
  const dummyProgram = new parser(
    Array.from(
      new Set(
        getCueAtoms(parser.cachedHandParsersProgramRoot) // is there a better method name than this?
      )
    ).join("\n"), this.getLine()
  )
  // Delete any particles that are not measures
  dummyProgram.filter(particle => !particle.isMeasure).forEach(particle => particle.destroy())
  dummyProgram.forEach(particle => {
    // add nested measures
    Object.keys(particle.definition.cueMapWithDefinitions).forEach(key => particle.appendLine(key))
  })
  // Delete any nested particles that are not measures
  dummyProgram.topDownArray.filter(particle => !particle.isMeasure).forEach(particle => particle.destroy())
  const measures = dummyProgram.topDownArray.map(particle => {
    return {
      Name: particle.measureName,
      Values: 0,
      Coverage: 0,
      Question: particle.definition.description,
      Example: particle.definition.getParticle("example")?.subparticlesToString() || "",
      Type: particle.typeForWebForms,
      Source: particle.sourceDomain,
      //Definition: parsedProgram.root.filename + ":" + particle.lineNumber
      SortIndex: particle.sortIndex,
      IsComputed: particle.isComputed,
      IsRequired: particle.isMeasureRequired,
      IsConceptDelimiter: particle.isConceptDelimiter,
      Cue: particle.definition.get("cue")
    }
  })
  measureCache.set(parser, lodash.sortBy(measures, "SortIndex"))
  return measureCache.get(parser)
  }
  _concepts
  get concepts() {
    if (this._concepts) return this._concepts
    this._concepts = this.parseConcepts(this, this.measures)
    return this._concepts
  }
  _measures
  get measures() {
    if (this._measures) return this._measures
    this._measures = this.parseMeasures(this.parser)
    return this._measures
  }
  parseConcepts(parsedProgram, measures){
  // Todo: might be a perf/memory/simplicity win to have a "segment" method in ScrollSDK, where you could
  // virtually split a Particle into multiple segments, and then query on those segments.
  // So we would "segment" on "id ", and then not need to create a bunch of new objects, and the original
  // already parsed lines could then learn about/access to their respective segments.
  const conceptDelimiter = measures.filter(measure => measure.IsConceptDelimiter)[0]
  if (!conceptDelimiter) return []
  const concepts = parsedProgram.split(conceptDelimiter.Cue || conceptDelimiter.Name)
  concepts.shift() // Remove the part before "id"
  return concepts.map(concept => {
    const row = {}
    measures.forEach(measure => {
      const measureName = measure.Name
      const measureKey = measure.Cue || measureName.replace(/_/g, " ")
      if (!measure.IsComputed) row[measureName] = concept.getParticle(measureKey)?.measureValue ?? ""
      else row[measureName] = this.computeMeasure(parsedProgram, measureName, concept, concepts)
    })
    return row
  })
  }
  computeMeasure(parsedProgram, measureName, concept, concepts){
  // note that this is currently global, assuming there wont be. name conflicts in computed measures in a single scroll
  if (!Particle.measureFnCache) Particle.measureFnCache = {}
  const measureFnCache = Particle.measureFnCache
  if (!measureFnCache[measureName]) {
    // a bit hacky but works??
    const particle = parsedProgram.appendLine(measureName)
    measureFnCache[measureName] = particle.computeValue
    particle.destroy()
  }
  return measureFnCache[measureName](concept, measureName, parsedProgram, concepts)
  }
  compileMeasures(filename = "csv", sortBy = "") {
    const withStats = this.measuresWithStats
    if (!sortBy) return this._compileArray(filename, withStats)
    const orderBy = this.makeLodashOrderByParams(sortBy)
    return this._compileArray(filename, this.lodash.orderBy(withStats, orderBy[0], orderBy[1]))
  }
  toRss() {
    const { title, canonicalUrl } = this
    return ` <item>
  <title>${title}</title>
  <link>${canonicalUrl}</link>
  <pubDate>${this.dayjs(this.timestamp * 1000).format("ddd, DD MMM YYYY HH:mm:ss ZZ")}</pubDate>
  </item>`
  }
 example
  # Hello world
  ## This is Scroll
  * It compiles to HTML.
  
  code
   // You can add code as well.
   print("Hello world")

abstractUrlSettingParser
 extends abstractTopLevelSingleMetaParser
 atoms metaCommandAtom urlAtom
 cueFromId
