abstractScrollParser
 atoms cueAtom
 javascript
  compileEmbeddedVersion(compileSettings) {
   return this.compile(compileSettings)
  }
  compileTxt() {
    return ""
  }
  getHtmlRequirements(compileSettings) {
    const {requireOnce} = this
    if (!requireOnce)
      return ""
    const set = compileSettings?.alreadyRequired || this.root.alreadyRequired
    if (set.has(requireOnce))
      return ""
    
    set.add(requireOnce)
    return requireOnce + "\n\n"
  }

abstractScrollWithRequirementsParser
 extends abstractScrollParser
 cueFromId
 javascript
  compile(compileSettings) {
    return this.getHtmlRequirements(compileSettings) + this.compileInstance()
  }

metaCommandAtom
 extends cueAtom
 description Give meta command atoms their own color.
 paint constant.numeric
 // Obviously this is not numeric. But I like the green color for now.
   We need a better design to replace this "paint" concept
   https://github.com/breck7/scrollsdk/issues/186

abstractTopLevelSingleMetaParser
 description Use these parsers once per file.
 extends abstractScrollParser
 inScope slashCommentParser
 cueFromId
 atoms metaCommandAtom
 javascript
  isTopMatter = true
  isSetterParser = true
  compile() {
   return ""
  }

scrollParser
 extensions scroll
 description Scroll is a language for scientists of all ages. Refine, share and collaborate on ideas.
 root
 inScope abstractScrollParser blankLineParser atomTypeDefinitionParser parserDefinitionParser
 catchAllParser catchAllParagraphParser
 compilesTo html
 javascript
  setFile(file) {
   this.file = file
   return this
  }
  compile(compileSettings) {
    this.sectionStack = []
    return this.map(subparticle => subparticle.compile(compileSettings)).filter(i => i).join("\n") + this.clearSectionStack()
  }
  sectionStack = []
  clearSectionStack() {
   const result = this.sectionStack.join("\n")
   this.sectionStack = []
   return result
  }
  bodyStack = []
  clearBodyStack() {
   const result = this.bodyStack.join("")
   this.bodyStack = []
   return result
  }
  get hakonParser() {
    if (this.isNodeJs())
      return require("scrollsdk/products/hakon.nodejs.js")
    return hakonParser
  }
  readSyncFromFileOrUrl(fileOrUrl) {
    if (!this.isNodeJs()) return localStorage.getItem(fileOrUrl) || ""
    const isUrl = fileOrUrl.match(/^https?\:[^ ]+$/)
    if (!isUrl) return this.root.readFile(fileOrUrl)
    return this.readFile(this.makeFullPath(new URL(fileOrUrl).pathname.split('/').pop()))
  }
  async fetch(url, filename) {
    const isUrl = url.match(/^https?\:[^ ]+$/)
    if (!isUrl) return
    return this.isNodeJs() ? this.fetchNode(url, filename) : this.fetchBrowser(url)
  }
  get path() {
    return require("path")
  }
  makeFullPath(filename) {
    return this.path.join(this.folderPath, filename)
  }
  get date() {
    return this.get("date") || this.file.date
  }
  get linkToPrevious() {
    return this.file.linkToPrevious
  }
  get linkToNext() {
    return this.file.linkToNext
  }
  async fetchNode(url, filename) {
    filename = filename || new URL(url).pathname.split('/').pop()
    const fullpath = this.makeFullPath(filename)
    if (require("fs").existsSync(fullpath)) return this.readFile(fullpath)
    this.log(`ðŸ›œ fetching ${url} to ${fullpath} `)
    await this.downloadToDisk(url, fullpath)
    return this.readFile(fullpath)
  }
  log(message) {
    this.file.log ? this.file.log(message) : ""
  }
  async fetchBrowser(url) {
    const content = localStorage.getItem(url)
    if (content) return content
    return this.downloadToLocalStorage(url)
  }
  async downloadToDisk(url, destination) {
    const { writeFile } = require('fs').promises
    const response = await fetch(url)
    const fileBuffer = await response.arrayBuffer()
    await writeFile(destination, Buffer.from(fileBuffer))
    return this.readFile(destination)
  }
  async downloadToLocalStorage(url) {
    const response = await fetch(url)
    const blob = await response.blob()
    localStorage.setItem(url, await blob.text())
    return localStorage.getItem(url)
  }
  readFile(filename) {
    const {path} = this
    const fs = require("fs")
    const fullPath = path.join(this.folderPath, filename.replace(this.folderPath, ""))
    if (fs.existsSync(fullPath))
      return fs.readFileSync(fullPath, "utf8")
    console.error(`File '${filename}' not found`)
    return ""
  }
  alreadyRequired = new Set()
  compileEmbeddedVersion(compileSettings) {
   this.sectionStack = []
   return this.map(subparticle => (subparticle.compileEmbeddedVersion ? subparticle.compileEmbeddedVersion(compileSettings) : subparticle.compile(compileSettings)))
     .filter(i => i)
     .join("\n")
     .trim() + this.clearSectionStack()
  }
  get footnotes() {
   if (this._footnotes === undefined) this._footnotes = this.filter(particle => particle.isFootnote)
   return this._footnotes
  }
  get authors() {
    return this.get("authors")
  }
  async build() {
    await Promise.all(this.filter(particle => particle.build).map(async particle => particle.build()))
  }
  file = {}
  getFromParserId(parserId) {
    return this.parserIdIndex[parserId]?.[0].content
  }
  get fileSystem() {
    return this.file.fileSystem
  }
  get filePath() {
    return this.file.filePath
  }
  get folderPath() {
    return this.file.folderPath
  }
  get filename() {
    return this.file.filename || ""
  }
  get hasKeyboardNav() {
    return this.has("keyboardNav")
  }
  get editHtml() {
    return `<a href="${this.editUrl}" class="abstractTextLinkParser">Edit</a>`
  }
  get externalsPath() {
    return this.file.EXTERNALS_PATH
  }
  get allScrollFiles() {
    return this.file.allScrollFiles || []
  }
  getFilesByTags(tags) {
    return this.file.getFilesByTags ? this.file.getFilesByTags(tags) : []
  }
  get endSnippetIndex() {
    // Get the line number that the snippet should stop at.
    // First if its hard coded, use that
    if (this.has("endSnippet")) return this.getParticle("endSnippet").index
    // Next look for a dinkus
    const snippetBreak = this.find(particle => particle.isDinkus)
    if (snippetBreak) return snippetBreak.index
    return -1
  }
  get parserIds() {
    return this.topDownArray.map(particle => particle.definition.id)
  }
  get tags() {
    return this.get("tags") || ""
  }
  get primaryTag() {
    return this.tags.split(" ")[0]
  }
  get filenameNoExtension() {
    return this.filename.replace(".scroll", "")
  }
  // todo: rename publishedUrl? Or something to indicate that this is only for stuff on the web (not localhost)
  // BaseUrl must be provided for RSS Feeds and OpenGraph tags to work
  get baseUrl() {
    const baseUrl = (this.get("baseUrl") || "").replace(/\/$/, "")
    return baseUrl + "/"
  }
  get canonicalUrl() {
    return this.get("canonicalUrl") || this.baseUrl + this.permalink
  }
  get openGraphImage() {
    const openGraphImage = this.get("openGraphImage")
    if (openGraphImage !== undefined) return this.ensureAbsoluteLink(openGraphImage)
    const images = this.filter(particle => particle.doesExtend("scrollImageParser"))
    const hit = images.find(particle => particle.has("openGraph")) || images[0]
    if (!hit) return ""
    return this.ensureAbsoluteLink(hit.filename)
  }
  get absoluteLink() {
    return this.ensureAbsoluteLink(this.permalink)
  }
  ensureAbsoluteLink(link) {
    if (link.includes("://")) return link
    return this.baseUrl + link.replace(/^\//, "")
  }
  get editUrl() {
    const editUrl = this.get("editUrl")
    if (editUrl) return editUrl
    const editBaseUrl = this.get("editBaseUrl")
    return (editBaseUrl ? editBaseUrl.replace(/\/$/, "") + "/" : "") + this.filename
  }
  get gitRepo() {
    // given https://github.com/breck7/breckyunits.com/blob/main/four-tips-to-improve-communication.scroll
    // return https://github.com/breck7/breckyunits.com
    return this.editUrl.split("/").slice(0, 5).join("/")
  }
  get scrollVersion() {
    return this.file.SCROLL_VERSION
  }
  // Use the first paragraph for the description
  // todo: add a particle method version of get that gets you the first particle. (actulaly make get return array?)
  // would speed up a lot.
  get description() {
    const description = this.getFromParserId("openGraphDescriptionParser")
    if (description) return description
    return this.generatedDescription
  }
  get generatedDescription() {
    const firstParagraph = this.find(particle => particle.isArticleContent)
    return firstParagraph ? firstParagraph.originalText.substr(0, 100).replace(/[&"<>']/g, "") : ""
  }
  get titleFromFilename() {
    const unCamelCase = str => str.replace(/([a-z])([A-Z])/g, "$1 $2").replace(/^./, match => match.toUpperCase())
    return unCamelCase(this.filenameNoExtension)
  }
  get title() {
    return this.getFromParserId("scrollTitleParser") || this.titleFromFilename
  }
  get linkTitle() {
    return this.getFromParserId("scrollLinkTitleParser") || this.title
  }
  get permalink() {
   return this.get("permalink") || (this.filename ? this.filenameNoExtension + ".html" : "")
  }
  compileTo(extensionCapitalized) {
    if (extensionCapitalized === "Txt")
      return this.asTxt
    if (extensionCapitalized === "Html")
      return this.asHtml
    const methodName = "compile" + extensionCapitalized
    return this.topDownArray
      .filter(particle => particle[methodName])
      .map((particle, index) => particle[methodName](index))
      .join("\n")
      .trim()
  }
  get asTxt() {
    return (
      this.map(particle => {
          const text = particle.compileTxt ? particle.compileTxt() : ""
          if (text) return text + "\n"
          if (!particle.getLine().length) return "\n"
          return ""
        })
        .join("")
        .replace(/<[^>]*>/g, "")
        .replace(/\n\n\n+/g, "\n\n") // Maximum 2 newlines in a row
        .trim() + "\n" // Always end in a newline, Posix style
    )
  }
  get dependencies() {
      const dependencies = this.file.dependencies?.slice() || []
      const files = this.topDownArray.filter(particle => particle.dependencies).map(particle => particle.dependencies).flat()
      return dependencies.concat(files)
  }
  get buildsHtml() {
    const { permalink } = this
    return !this.file?.importOnly && (permalink.endsWith(".html") || permalink.endsWith(".htm"))
  }
  // Without specifying the language hyphenation will not work.
  get lang() {
    return this.get("htmlLang") || "en"
  }
  _compiledHtml = ""
  get asHtml() {
    if (!this._compiledHtml) {
      const { permalink, buildsHtml } = this
      const content = (this.compile() + this.clearBodyStack()).trim()
      // Don't add html tags to CSV feeds. A little hacky as calling a getter named _html_ to get _xml_ is not ideal. But
      // <1% of use case so might be good enough.
      const wrapWithHtmlTags = buildsHtml
      const bodyTag = this.has("metaTags") ? "" : "<body>\n"
      this._compiledHtml = wrapWithHtmlTags ? `<!DOCTYPE html>\n<html lang="${this.lang}">\n${bodyTag}${content}\n</body>\n</html>` : content
    }
    return this._compiledHtml
  }
  get wordCount() {
    return this.asTxt.match(/\b\w+\b/g)?.length || 0
  }
  get minutes() {
    return parseFloat((this.wordCount / 200).toFixed(1))
  }
  get date() {
    const date = this.get("date") || (this.file.timestamp ? this.file.timestamp * 1000 : 0) || 0
    return this.dayjs(date).format(`MM/DD/YYYY`)
  }
  get year() {
    return parseInt(this.dayjs(this.date).format(`YYYY`))
  }
  get dayjs() {
    if (!this.isNodeJs()) return dayjs
    const lib = require("dayjs")
    const relativeTime = require("dayjs/plugin/relativeTime")
    lib.extend(relativeTime)
    return lib
  }
  get lodash() {
    return this.isNodeJs() ? require("lodash") : lodash
  }
  getConcepts(parsed) {
    const concepts = []
    let currentConcept
    parsed.forEach(particle => {
      if (particle.isConceptDelimiter) {
        if (currentConcept) concepts.push(currentConcept)
        currentConcept = []
      }
      if (currentConcept && particle.isMeasure) currentConcept.push(particle)
    })
    if (currentConcept) concepts.push(currentConcept)
    return concepts
  }
  _formatConcepts(parsed) {
    const concepts = this.getConcepts(parsed)
    if (!concepts.length) return false
    const {lodash} = this
    // does a destructive sort in place on the parsed program
    concepts.forEach(concept => {
      let currentSection
      const newCode = lodash
        .sortBy(concept, ["sortIndex"])
        .map(particle => {
          let newLines = ""
          const section = particle.sortIndex.toString().split(".")[0]
          if (section !== currentSection) {
            currentSection = section
            newLines = "\n"
          }
          return newLines + particle.toString()
        })
        .join("\n")
      concept.forEach((particle, index) => (index ? particle.destroy() : ""))
      concept[0].replaceParticle(() => newCode)
    })
  }
  getFormatted(codeAtStart = this.toString()) {
    let formatted = codeAtStart.replace(/\r/g, "") // remove all carriage returns if there are any
    const parsed = new this.constructor(formatted)
    parsed.topDownArray.forEach(subparticle => {
      subparticle.format()
      const original = subparticle.getLine()
      const trimmed = original.replace(/(\S.*?)[  \t]*$/gm, "$1")
      // Trim trailing whitespace unless parser allows it
      if (original !== trimmed && !subparticle.allowTrailingWhitespace) subparticle.setLine(trimmed)
    })
    this._formatConcepts(parsed)
    let importOnlys = []
    let topMatter = []
    let allElse = []
    // Create any bindings
    parsed.forEach(particle => {
      if (particle.bindTo === "next") particle.binding = particle.next
      if (particle.bindTo === "previous") particle.binding = particle.previous
    })
    parsed.forEach(particle => {
      if (particle.getLine() === "importOnly") importOnlys.push(particle)
      else if (particle.isTopMatter) topMatter.push(particle)
      else allElse.push(particle)
    })
    const combined = importOnlys.concat(topMatter, allElse)
    // Move any bound particles
    combined
      .filter(particle => particle.bindTo)
      .forEach(particle => {
        // First remove the particle from its current position
        const originalIndex = combined.indexOf(particle)
        combined.splice(originalIndex, 1)
        // Then insert it at the new position
        // We need to find the binding index again after removal
        const bindingIndex = combined.indexOf(particle.binding)
        if (particle.bindTo === "next") combined.splice(bindingIndex, 0, particle)
        else combined.splice(bindingIndex + 1, 0, particle)
      })
    const trimmed = combined
      .map(particle => particle.toString())
      .join("\n")
      .replace(/^\n*/, "") // Remove leading newlines
      .replace(/\n\n\n+/g, "\n\n") // Maximum 2 newlines in a row
      .replace(/\n+$/, "")
    return trimmed === "" ? trimmed : trimmed + "\n" // End non blank Scroll files in a newline character POSIX style for better working with tools like git
  }
  get parser() {
    return this.constructor
  }
  get parsersRequiringExternals() {
    const { parser } = this
    // todo: could be cleaned up a bit
    if (!parser.parsersRequiringExternals) parser.parsersRequiringExternals = parser.cachedHandParsersProgramRoot.filter(particle => particle.copyFromExternal).map(particle => particle.atoms[0])
    return parser.parsersRequiringExternals
  }
  get Disk() { return this.isNodeJs() ? require("scrollsdk/products/Disk.node.js").Disk : {}}
  _copyExternalFiles(externalFilesCopied = {}) {
    if (!this.isNodeJs()) return
    // If this file uses a parser that has external requirements,
    // copy those from external folder into the destination folder.
    const { parsersRequiringExternals, folderPath, fileSystem, filename, parserIdIndex, path } = this
    const { Disk } = this
    if (!externalFilesCopied[folderPath]) externalFilesCopied[folderPath] = {}
    parsersRequiringExternals.forEach(parserId => {
      if (externalFilesCopied[folderPath][parserId]) return
      if (!parserIdIndex[parserId]) return
      parserIdIndex[parserId].map(particle => {
        const externalFiles = particle.copyFromExternal.split(" ")
        externalFiles.forEach(name => {
          const newPath = path.join(folderPath, name)
          fileSystem.writeProduct(newPath, Disk.read(path.join(this.externalsPath, name)))
          this.log(`ðŸ’¾ Copied external file needed by ${filename} to ${name}`)
        })
      })
      if (parserId !== "scrollThemeParser")
        // todo: generalize when not to cache
        externalFilesCopied[folderPath][parserId] = true
    })
  }
  _buildFileType(extension) {
    const { fileSystem, folderPath, filename, filePath, path } = this
    const capitalized = this.lodash.capitalize(extension)
    const buildKeyword = "build" + capitalized
    if (!this.has(buildKeyword)) return
    const { permalink } = this
    const outputFiles = this.get(buildKeyword)?.split(" ") || [""]
    outputFiles.forEach(name => {
      const link = name || permalink.replace(".html", "." + extension)
      try {
        fileSystem.writeProduct(path.join(folderPath, link), this.compileTo(capitalized))
        this.log(`ðŸ’¾ Built ${link} from ${filename}`)
      } catch (err) {
        console.error(`Error while building '${filePath}' with extension '${extension}'`)
        throw err
      }
    })
  }
  async buildPdf() {
    if (!this.isNodeJs()) return "Only works in Node currently."
    const { filename } = this
    const outputFile = this.filenameNoExtension + ".pdf"
    // relevant source code for chrome: https://github.com/chromium/chromium/blob/a56ef4a02086c6c09770446733700312c86f7623/components/headless/command_handler/headless_command_switches.cc#L22
    const command = `/Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --headless --disable-gpu --no-pdf-header-footer --default-background-color=00000000 --no-pdf-background --print-to-pdf="${outputFile}" "${this.permalink}"`
    // console.log(`Node.js is running on architecture: ${process.arch}`)
    try {
      const output = require("child_process").execSync(command, { stdio: "ignore" })
      this.log(`ðŸ’¾ Built ${outputFile} from ${filename}`)
    } catch (error) {
      console.error(error)
    }
  }
  async buildOne() {
    // todo: cleanup
    // todo: iterate over buildFile particles directly. not this hard coded order.
    await this.build()
    this._buildFileType("parsers")
    this._buildConceptsAndMeasures() // todo: call this buildDelimited?
    this._buildFileType("csv")
    this._buildFileType("tsv")
    this._buildFileType("json")
  }
  async buildTwo(externalFilesCopied = {}) {
    // todo: iterate over buildFile particles directly. not this hard coded order.
    if (this.has("buildHtml")) this._copyExternalFiles(externalFilesCopied)
    this._buildFileType("js")
    this._buildFileType("txt")
    this._buildFileType("html")
    this._buildFileType("rss")
    this._buildFileType("css")
    if (this.has("buildPdf")) this.buildPdf()
  }
  _buildConceptsAndMeasures() {
    const { fileSystem, folderPath, filename, path } = this
    // If this proves useful maybe make slight adjustments to Scroll lang to be more imperative.
    if (!this.has("buildConcepts")) return
    const { permalink } = this
    this.findParticles("buildConcepts").forEach(particle => {
      const files = particle.getAtomsFrom(1)
      if (!files.length) files.push(permalink.replace(".html", ".csv"))
      const sortBy = particle.get("sortBy")
      files.forEach(link => {
        fileSystem.writeProduct(path.join(folderPath, link), this.compileConcepts(link, sortBy))
        this.log(`ðŸ’¾ Built concepts in ${filename} to ${link}`)
      })
    })
    if (!this.has("buildMeasures")) return
    this.findParticles("buildMeasures").forEach(particle => {
      const files = particle.getAtomsFrom(1)
      if (!files.length) files.push(permalink.replace(".html", ".csv"))
      const sortBy = particle.get("sortBy")
      files.forEach(link => {
        fileSystem.writeProduct(path.join(folderPath, link), this.compileMeasures(link, sortBy))
        this.log(`ðŸ’¾ Built measures in ${filename} to ${link}`)
      })
    })
  }
  _compileArray(filename, arr) {
    const removeBlanks = data => data.map(obj => Object.fromEntries(Object.entries(obj).filter(([_, value]) => value !== "")))
    const parts = filename.split(".")
    const format = parts.pop()
    if (format === "json") return JSON.stringify(removeBlanks(arr), null, 2)
    if (format === "js") return `const ${parts[0]} = ` + JSON.stringify(removeBlanks(arr), null, 2)
    if (format === "csv") return this.arrayToCSV(arr)
    if (format === "tsv") return this.arrayToCSV(arr, "\t")
    if (format === "particles") return particles.toString()
    return particles.toString()
  }
  makeLodashOrderByParams(str) {
  const part1 = str.split(" ")
  const part2 = part1.map(col => (col.startsWith("-") ? "desc" : "asc"))
  return [part1.map(col => col.replace(/^\-/, "")), part2]
  }
  arrayToCSV(data, delimiter = ",") {
   if (!data.length) return ""
   // Extract headers
   const headers = Object.keys(data[0])
   const csv = data.map(row =>
     headers
       .map(fieldName => {
         const fieldValue = row[fieldName]
         // Escape commas if the value is a string
         if (typeof fieldValue === "string" && fieldValue.includes(delimiter)) {
           return `"${fieldValue.replace(/"/g, '""')}"` // Escape double quotes and wrap in double quotes
         }
         return fieldValue
       })
       .join(delimiter)
   )
   csv.unshift(headers.join(delimiter)) // Add header row at the top
   return csv.join("\n")
   }
  compileConcepts(filename = "csv", sortBy = "") {
    const {lodash} = this
    if (!sortBy) return this._compileArray(filename, this.concepts)
    const orderBy = this.makeLodashOrderByParams(sortBy)
    return this._compileArray(filename, lodash.orderBy(this.concepts, orderBy[0], orderBy[1]))
  }
  _withStats
  get measuresWithStats() {
    if (!this._withStats) this._withStats = this.addMeasureStats(this.concepts, this.measures)
    return this._withStats
  }
  addMeasureStats(concepts, measures){
   return measures.map(measure => {
    let Type = false
    concepts.forEach(concept => {
      const value = concept[measure.Name]
      if (value === undefined || value === "") return
      measure.Values++
      if (!Type) {
        measure.Example = value.toString().replace(/\n/g, " ")
        measure.Type = typeof value
        Type = true
      }
    })
    measure.Coverage = Math.floor((100 * measure.Values) / concepts.length) + "%"
    return measure
  })
  }
  parseMeasures(parser) {
  if (!Particle.measureCache)
    Particle.measureCache = new Map()
  const measureCache = Particle.measureCache
  if (measureCache.get(parser)) return measureCache.get(parser)
  const {lodash} = this
  // todo: clean this up
  const getCueAtoms = rootParserProgram =>
    rootParserProgram
      .filter(particle => particle.getLine().endsWith("Parser") && !particle.getLine().startsWith("abstract"))
      .map(particle => particle.get("cue") || particle.getLine())
      .map(line => line.replace(/Parser$/, ""))
  // Generate a fake program with one of every of the available parsers. Then parse it. Then we can easily access the meta data on the parsers
  const dummyProgram = new parser(
    Array.from(
      new Set(
        getCueAtoms(parser.cachedHandParsersProgramRoot) // is there a better method name than this?
      )
    ).join("\n")
  )
  // Delete any particles that are not measures
  dummyProgram.filter(particle => !particle.isMeasure).forEach(particle => particle.destroy())
  dummyProgram.forEach(particle => {
    // add nested measures
    Object.keys(particle.definition.cueMapWithDefinitions).forEach(key => particle.appendLine(key))
  })
  // Delete any nested particles that are not measures
  dummyProgram.topDownArray.filter(particle => !particle.isMeasure).forEach(particle => particle.destroy())
  const measures = dummyProgram.topDownArray.map(particle => {
    return {
      Name: particle.measureName,
      Values: 0,
      Coverage: 0,
      Question: particle.definition.description,
      Example: particle.definition.getParticle("example")?.subparticlesToString() || "",
      Type: particle.typeForWebForms,
      Source: particle.sourceDomain,
      //Definition: parsedProgram.root.filename + ":" + particle.lineNumber
      SortIndex: particle.sortIndex,
      IsComputed: particle.isComputed,
      IsRequired: particle.isMeasureRequired,
      IsConceptDelimiter: particle.isConceptDelimiter,
      Cue: particle.definition.get("cue")
    }
  })
  measureCache.set(parser, lodash.sortBy(measures, "SortIndex"))
  return measureCache.get(parser)
  }
  _concepts
  get concepts() {
    if (this._concepts) return this._concepts
    this._concepts = this.parseConcepts(this, this.measures)
    return this._concepts
  }
  _measures
  get measures() {
    if (this._measures) return this._measures
    this._measures = this.parseMeasures(this.parser)
    return this._measures
  }
  parseConcepts(parsedProgram, measures){
  // Todo: might be a perf/memory/simplicity win to have a "segment" method in ScrollSDK, where you could
  // virtually split a Particle into multiple segments, and then query on those segments.
  // So we would "segment" on "id ", and then not need to create a bunch of new objects, and the original
  // already parsed lines could then learn about/access to their respective segments.
  const conceptDelimiter = measures.filter(measure => measure.IsConceptDelimiter)[0]
  if (!conceptDelimiter) return []
  const concepts = parsedProgram.split(conceptDelimiter.Cue || conceptDelimiter.Name)
  concepts.shift() // Remove the part before "id"
  return concepts.map(concept => {
    const row = {}
    measures.forEach(measure => {
      const measureName = measure.Name
      const measureKey = measure.Cue || measureName.replace(/_/g, " ")
      if (!measure.IsComputed) row[measureName] = concept.getParticle(measureKey)?.measureValue ?? ""
      else row[measureName] = this.computeMeasure(parsedProgram, measureName, concept, concepts)
    })
    return row
  })
  }
  computeMeasure(parsedProgram, measureName, concept, concepts){
  // note that this is currently global, assuming there wont be. name conflicts in computed measures in a single scroll
  if (!Particle.measureFnCache) Particle.measureFnCache = {}
  const measureFnCache = Particle.measureFnCache
  if (!measureFnCache[measureName]) {
    // a bit hacky but works??
    const particle = parsedProgram.appendLine(measureName)
    measureFnCache[measureName] = particle.computeValue
    particle.destroy()
  }
  return measureFnCache[measureName](concept, measureName, parsedProgram, concepts)
  }
  compileMeasures(filename = "csv", sortBy = "") {
    const withStats = this.measuresWithStats
    if (!sortBy) return this._compileArray(filename, withStats)
    const orderBy = this.makeLodashOrderByParams(sortBy)
    return this._compileArray(filename, this.lodash.orderBy(withStats, orderBy[0], orderBy[1]))
  }
  async buildAll() {
    await this.buildOne()
    this.buildTwo()
  }
  evalNodeJsMacros(value, macroMap, filePath) {
    const tempPath = filePath + ".js"
    const {Disk} = this
    if (Disk.exists(tempPath)) throw new Error(`Failed to write/require replaceNodejs snippet since '${tempPath}' already exists.`)
    try {
      Disk.write(tempPath, value)
      const results = require(tempPath)
      Object.keys(results).forEach(key => (macroMap[key] = results[key]))
    } catch (err) {
      console.error(`Error in evalMacros in file '${filePath}'`)
      console.error(err)
    } finally {
      Disk.rm(tempPath)
    }
  }
  parseAndCompile(codeAfterImportPass, codeAtStart, absoluteFilePath, parser){
    // PASS 3: READ AND REPLACE MACROS. PARSE AND REMOVE MACROS DEFINITIONS THEN REPLACE REFERENCES.
    const codeAfterMacroPass = this.evalMacros(codeAfterImportPass, codeAtStart, absoluteFilePath)
    // PASS 4: READ WITH STD COMPILER OR CUSTOM COMPILER.
    return {
      codeAfterMacroPass,
      parser,
      scrollProgram: new parser(codeAfterMacroPass)
    }
  }
  evalMacros(code, codeAtStart, absolutePath) {
    // note: the 2 params above are not used in this method, but may be used in user eval code. (todo: cleanup)
    const regex = /^(replace|footer$)/gm
    if (!regex.test(code)) return code
    const particle = new Particle(code) // todo: this can be faster. a more lightweight particle class?
    // Process macros
    const macroMap = {}
    particle
      .filter(particle => {
        const parserAtom = particle.cue
        return parserAtom === "replace" || parserAtom === "replaceJs" || parserAtom === "replaceNodejs"
      })
      .forEach(particle => {
        let value = particle.length ? particle.subparticlesToString() : particle.getAtomsFrom(2).join(" ")
        const kind = particle.cue
        if (kind === "replaceJs") value = eval(value)
        if (this.isNodeJs() && kind === "replaceNodejs")
          this.evalNodeJsMacros(value, macroMap, absolutePath)
        else macroMap[particle.getAtom(1)] = value
        particle.destroy() // Destroy definitions after eval
      })
    if (particle.has("footer")) {
      const pushes = particle.getParticles("footer")
      const append = pushes.map(push => push.section.join("\n")).join("\n")
      pushes.forEach(push => {
        push.section.forEach(particle => particle.destroy())
        push.destroy()
      })
      code = particle.asString + append
    }
    const keys = Object.keys(macroMap)
    if (!keys.length) return code
    let codeAfterMacroSubstitution = particle.asString
    // Todo: speed up. build a template?
    Object.keys(macroMap).forEach(key => (codeAfterMacroSubstitution = codeAfterMacroSubstitution.replace(new RegExp(key, "g"), macroMap[key])))
    return codeAfterMacroSubstitution
  }
  toRss() {
    const { title, canonicalUrl } = this
    return ` <item>
  <title>${title}</title>
  <link>${canonicalUrl}</link>
  <pubDate>${this.dayjs(this.timestamp * 1000).format("ddd, DD MMM YYYY HH:mm:ss ZZ")}</pubDate>
  </item>`
  }
 example
  # Hello world
  ## This is Scroll
  * It compiles to HTML.
  
  code
   // You can add code as well.
   print("Hello world")

abstractUrlSettingParser
 extends abstractTopLevelSingleMetaParser
 atoms metaCommandAtom urlAtom
 cueFromId
